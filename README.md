Speculative decoding inference frameworks have been developed to work around the memory-bound bottlenecks of standard autoregressive decoding in LLM's, employing a smaller model or model heads that propose multiple candidate tokens, which are then verified and decoded by the target model at once during a singular decoding step. One such framework is Medusa, where multiple small draft heads are fed the base target model's hidden states and are trained to predict candidates for token sequences. As the number of candidate sequences and length of candidate sequences increase, acceptance lengths increase, but at a cost of increased overhead, since Medusa decoding becomes compute-bound, with computationally expensive attention mechanisms in the verification step. This is especially suboptimal for tasks that involve less deterministic LLM responses, such as writing or reasoning, whose lower candidate acceptance rates don't reap the same benefits of longer candidate sequences that more deterministic tasks such as coding or extraction do, all while having to pay the same overhead cost. We propose integrating adaptive speculative decoding into Medusa by allowing each decoding step to have a variable number of draft heads used in the computationally expensive candidate verification, that adapts based on an entropy heuristic from the candidate proposal phase, minimizing the wasted overhead of verifying candidates that are unlikely to be accepted. We achieve $1.4\%$ speedup on Vicuna-7b and $2.5\%$ speedup on Vicuna-33b on MT-Bench, demonstrating potential for an adaptive speculative decoding approach to bring improvement on larger models and incorporate more Medusa heads, especially on higher variance tasks which experience up to $4.3\%$ speedup.
